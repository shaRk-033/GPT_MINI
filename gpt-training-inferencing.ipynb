{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8514979,"sourceType":"datasetVersion","datasetId":5083566}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import regex as re\n\nclass GPTTokenizer():\n    def __init__(self, vocab_size):\n        self.GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n        self.vocab_size = vocab_size\n        self.bigram_tree = {}\n        self.vocab = {idx: bytes([idx]) for idx in range(256)}\n\n    def split_text(self, text):\n        return re.findall(self.GPT4_SPLIT_PATTERN, text)\n\n    def freq(self, tokens, stats):\n        for id1, id2 in zip(tokens, tokens[1:]):\n            stats[(id1, id2)] = stats.get((id1, id2), 0) + 1\n\n    def replace(self, tokens, pair, idx):\n        newids = []\n        i = 0\n        while i < len(tokens):\n            if tokens[i] == pair[0] and i < len(tokens) - 1 and tokens[i+1] == pair[1]:\n                newids.append(idx)\n                i += 2\n            else:\n                newids.append(tokens[i])\n                i += 1\n        return newids\n\n    def train(self, text):\n        chunks = re.findall(self.GPT4_SPLIT_PATTERN, text)\n        tokens = [list(chunk.encode('utf-8')) for chunk in chunks]\n\n        for i in range(self.vocab_size - 256):\n            stats = {}\n            for token in tokens:\n                self.freq(token, stats)\n            maxi = max(stats, key=stats.get)\n            tokens = [self.replace(token, maxi, 256 + i) for token in tokens]\n            self.bigram_tree[maxi] = 256 + i\n            self.vocab[256 + i] = self.vocab[maxi[0]] + self.vocab[maxi[1]]\n\n    def encode_chunk(self, tokens):\n        while len(tokens)>=2:\n            stats = {}\n            self.freq(tokens, stats)\n            pair = min(stats, key = lambda p: self.bigram_tree.get(p, float(\"inf\")))\n            if pair not in self.bigram_tree:\n                break\n            tokens = self.replace(tokens, pair, self.bigram_tree[pair])\n        return tokens\n\n    def encode(self, text):\n        chunks = re.findall(self.GPT4_SPLIT_PATTERN, text)\n        tokens = []\n        for chunk in chunks:\n            tks = list(chunk.encode('utf-8'))\n            tks = self.encode_chunk(tks)\n            tokens.extend(tks)\n\n        return tokens\n\n    def decode(self, tokens):\n        x = (b\"\".join(self.vocab[idx] for idx in tokens))\n        text = x.decode('utf-8', errors=\"ignore\")\n        return text\n\n\ntext = \"\"\nwith open(\"/kaggle/input/shakespeare/input.txt\", 'r', encoding = 'utf-8') as f:\n    text = f.read()\n\ntokenizer = GPTTokenizer(1000)\ntokenizer.train(text)\n\n\n","metadata":{"id":"4cUnD4lfp2hs","execution":{"iopub.status.busy":"2024-05-26T06:15:21.939958Z","iopub.execute_input":"2024-05-26T06:15:21.940777Z","iopub.status.idle":"2024-05-26T06:28:18.662079Z","shell.execute_reply.started":"2024-05-26T06:15:21.940743Z","shell.execute_reply":"2024-05-26T06:28:18.660963Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport regex as re\nimport torch.optim as optim\ndata = torch.tensor(tokenizer.encode(text))\nvocab_size = len(tokenizer.vocab)\n\nn = int(len(data)*0.8)\ntrain = data[:n]\nvalid = data[n:]\n\ndef get_batch(specify, batch_size, block_size):\n    data = train if specify == 'train' else valid\n    idx = torch.randint(len(data)-block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in idx])\n    y = torch.stack([data[i+1:i+block_size+1] for i in idx])\n    return x,y\n# tokenizer.vocab","metadata":{"id":"oPDJUFWg-Rqk","execution":{"iopub.status.busy":"2024-05-26T06:28:18.664121Z","iopub.execute_input":"2024-05-26T06:28:18.664517Z","iopub.status.idle":"2024-05-26T06:28:24.344738Z","shell.execute_reply.started":"2024-05-26T06:28:18.664482Z","shell.execute_reply":"2024-05-26T06:28:24.343922Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"\nclass EmbeddingLayer(nn.Module):\n    def __init__(self, d_model, vocab_size, context_length, dropout) -> None:\n        super().__init__()\n        self.d_model = d_model\n        self.vocab_size = vocab_size\n        self.context_length = context_length\n        self.inp_embedding = nn.Embedding(self.vocab_size, self.d_model)\n        self.dropout = nn.Dropout(dropout)\n        posits = torch.zeros(self.context_length, self.d_model)\n        position = torch.arange(0, self.context_length, dtype=torch.float).unsqueeze(1)\n        v_emb = torch.arange(0, self.d_model, 2, dtype=torch.float)\n\n        posits[:, 0::2] = torch.sin(position / (10000 ** (v_emb / self.d_model)))\n        posits[:, 1::2] = torch.cos(position / (10000 ** (v_emb / self.d_model)))\n\n        posits = posits.unsqueeze(0)\n        self.register_buffer('posits', posits)\n\n    def forward(self, x):\n        x = self.inp_embedding(x)\n        x = x + self.posits[:, :x.size(1), :]\n        x = self.dropout(x)\n        return x\n\nclass ResidualLink(nn.Module):\n    def __init__(self, sublayer, d_model, dropout):\n        super().__init__()\n        self.sublayer = sublayer\n        self.norm = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        return x + self.dropout(self.sublayer(self.norm(x)))\n\nclass CausalSelfAttention(nn.Module):\n    def __init__(self, d_model, head_size, dropout) -> None:\n        super().__init__()\n        self.d_model = d_model\n        self.head_size = head_size\n        self.dropout = nn.Dropout(dropout)\n        self.query = nn.Linear(self.d_model, self.head_size)\n        self.key = nn.Linear(self.d_model, self.head_size)\n        self.value = nn.Linear(self.d_model, self.head_size)\n\n    def forward(self, x):\n        B, T, C = x.shape\n        q = self.query(x)\n        k = self.key(x)\n        v = self.value(x)\n        out1 = q @ k.transpose(-2, -1) / (self.head_size ** 0.5)\n        mask = torch.tril(torch.ones(T, T)).to(x.device)\n        out1 = out1.masked_fill(mask == 0, float('-inf'))\n        out1 = F.softmax(out1, dim=-1)\n        out1 = self.dropout(out1)\n        out2 = out1 @ v\n        return out2\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, num_heads, dropout) -> None:\n        super().__init__()\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.dropout = nn.Dropout(dropout)\n        self.head_size = self.d_model // self.num_heads\n        self.attention_heads = nn.ModuleList([CausalSelfAttention(self.d_model, self.head_size, dropout) for _ in range(self.num_heads)])\n        self.w_out = nn.Linear(self.d_model, self.d_model)\n\n    def forward(self, x):\n        self.out = torch.cat([head(x) for head in self.attention_heads], dim=-1)\n        self.out = self.w_out(self.out)\n        self.out = self.dropout(self.out)\n        return self.out\n\nclass FeedForwardNetwork(nn.Module):\n    def __init__(self, d_model, dropout) -> None:\n        super().__init__()\n        self.d_model = d_model\n        self.d_ff = d_model * 4\n        self.dropout = nn.Dropout(dropout)\n        self.linear1 = nn.Linear(self.d_model, self.d_ff)\n        self.linear2 = nn.Linear(self.d_ff, self.d_model)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        out = self.relu(self.linear1(x))\n        out = self.linear2(out)\n        out = self.dropout(out)\n        return out\n\nclass DecoderBlock(nn.Module):\n    def __init__(self, d_model, num_heads, dropout) -> None:\n        super().__init__()\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.dropout = nn.Dropout(dropout)\n        self.attention = ResidualLink(MultiHeadAttention(self.d_model, self.num_heads, dropout), self.d_model, dropout)\n        self.feedforward = ResidualLink(FeedForwardNetwork(self.d_model, dropout), self.d_model, dropout)\n\n    def forward(self, x):\n        x = self.attention(x)\n        x = self.feedforward(x)\n        return x\n\nclass GPTBlock(nn.Module):\n    def __init__(self, vocab_size, d_model, context_length, n_decoder_blocks, dropout) -> None:\n        super().__init__()\n        self.embeds = EmbeddingLayer(d_model, vocab_size, context_length, dropout)\n        self.decoder_blocks = nn.ModuleList([DecoderBlock(d_model, 8, dropout) for _ in range(n_decoder_blocks)])\n        self.norm = nn.LayerNorm(d_model)\n        self.linear = nn.Linear(d_model, vocab_size)\n\n    def forward(self, x):\n        x = self.embeds(x)\n        for block in self.decoder_blocks:\n            x = block(x)\n        x = self.norm(x)\n        x = self.linear(x)\n        return x\n\nclass BigramLM(nn.Module):\n    def __init__(self, context_length, n_decoder_blocks, dropout, vocab_size=1000, d_model=512):\n        super().__init__()\n        self.vocab_size = vocab_size\n        self.d_model = d_model\n        self.model = GPTBlock(vocab_size, d_model, context_length, n_decoder_blocks, dropout)\n        self.context_length = context_length\n\n    def forward(self, x, targets=None):\n        x = x.cuda()\n        logits = self.model(x)\n        loss = None\n        if targets is not None:\n            targets = targets.cuda()\n            logits = logits.view(-1, self.vocab_size)\n            targets = targets.view(-1)\n            loss = F.cross_entropy(logits, targets)\n        return logits, loss\n\n    @torch.no_grad()\n    def generate(self, x, num_char=1000):\n        for i in range(num_char):\n            x_cond = x[:, -self.context_length:]\n            pred, _ = self(x_cond)\n            preds = pred[:, -1, :]\n            probs = F.softmax(preds, dim=-1)\n            idx = torch.multinomial(probs, num_samples=1)\n            x = torch.cat((x, idx), dim=1)\n        return x","metadata":{"id":"isn0SNBmgqoA","execution":{"iopub.status.busy":"2024-05-26T06:28:24.346382Z","iopub.execute_input":"2024-05-26T06:28:24.346697Z","iopub.status.idle":"2024-05-26T06:28:24.381195Z","shell.execute_reply.started":"2024-05-26T06:28:24.346673Z","shell.execute_reply":"2024-05-26T06:28:24.380110Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"context_length = 64\nn_decoder_blocks = 8\nbatch_size = 64\neval_iters = 200\neval_interval = 2000\ndropout = 0.4","metadata":{"id":"8oxcVcasxyYU","execution":{"iopub.status.busy":"2024-05-26T06:28:24.383749Z","iopub.execute_input":"2024-05-26T06:28:24.384052Z","iopub.status.idle":"2024-05-26T06:28:24.395027Z","shell.execute_reply.started":"2024-05-26T06:28:24.384019Z","shell.execute_reply":"2024-05-26T06:28:24.394125Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"m = BigramLM(context_length, n_decoder_blocks, dropout)\nm = m.cuda()","metadata":{"id":"NCOxU6-bxJLE","execution":{"iopub.status.busy":"2024-05-26T06:28:24.396291Z","iopub.execute_input":"2024-05-26T06:28:24.396689Z","iopub.status.idle":"2024-05-26T06:28:24.892650Z","shell.execute_reply.started":"2024-05-26T06:28:24.396654Z","shell.execute_reply":"2024-05-26T06:28:24.891781Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    m.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split,batch_size,context_length)\n            logits, loss = m(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    m.train()\n    return out","metadata":{"id":"_bEaOB3l0t_X","execution":{"iopub.status.busy":"2024-05-26T06:28:24.893956Z","iopub.execute_input":"2024-05-26T06:28:24.894275Z","iopub.status.idle":"2024-05-26T06:28:24.900480Z","shell.execute_reply.started":"2024-05-26T06:28:24.894249Z","shell.execute_reply":"2024-05-26T06:28:24.899530Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FiglBzQ0xLdu","outputId":"d2bb5c83-0a26-473e-f164-8d08fb6e3fb7","execution":{"iopub.status.busy":"2024-05-26T06:28:24.901826Z","iopub.execute_input":"2024-05-26T06:28:24.902373Z","iopub.status.idle":"2024-05-26T06:28:24.917490Z","shell.execute_reply.started":"2024-05-26T06:28:24.902337Z","shell.execute_reply":"2024-05-26T06:28:24.916553Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"26.245096 M parameters\n","output_type":"stream"}]},{"cell_type":"code","source":"opt = optim.Adam(m.parameters(), lr = 3e-4)\n\nfor i in range(5000):\n    x,y = get_batch('train',batch_size, context_length)\n    preds,loss = m(x,y)\n    if i % eval_interval == 0:\n        losses = estimate_loss()\n        print(f\"step {i+20000}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n    opt.zero_grad()\n    loss.backward()\n    opt.step()\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":435},"id":"W0F-Y-VpxSv-","outputId":"48415014-0788-432e-b97f-fdf0edbb99b1","execution":{"iopub.status.busy":"2024-05-26T08:04:52.082521Z","iopub.execute_input":"2024-05-26T08:04:52.082938Z","iopub.status.idle":"2024-05-26T08:21:19.086458Z","shell.execute_reply.started":"2024-05-26T08:04:52.082907Z","shell.execute_reply":"2024-05-26T08:21:19.085572Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"step 20000: train loss 2.6248, val loss 4.2040\nstep 22000: train loss 2.4772, val loss 4.3019\nstep 24000: train loss 2.2124, val loss 4.3693\n","output_type":"stream"}]},{"cell_type":"code","source":"estimate_loss()","metadata":{"execution":{"iopub.status.busy":"2024-05-26T08:21:55.561813Z","iopub.execute_input":"2024-05-26T08:21:55.562202Z","iopub.status.idle":"2024-05-26T08:22:20.708150Z","shell.execute_reply.started":"2024-05-26T08:21:55.562171Z","shell.execute_reply":"2024-05-26T08:22:20.707124Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"{'train': tensor(2.0786), 'val': tensor(4.4397)}"},"metadata":{}}]},{"cell_type":"code","source":"sample_text = tokenizer.decode(list(map(int, m.generate(torch.zeros((1,1), dtype = torch.long).cuda(), 20000)[0])))","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":282},"id":"Cu2nerWrsoJP","outputId":"4a75e1c9-9613-4fd2-dc70-8b1f8f494311","execution":{"iopub.status.busy":"2024-05-26T08:22:26.681953Z","iopub.execute_input":"2024-05-26T08:22:26.682352Z","iopub.status.idle":"2024-05-26T08:32:35.446235Z","shell.execute_reply.started":"2024-05-26T08:22:26.682321Z","shell.execute_reply":"2024-05-26T08:32:35.445084Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"with open('sample.txt', 'w') as f:\n    f.write(sample_text)","metadata":{"id":"8mAutb3YM7uM","execution":{"iopub.status.busy":"2024-05-26T08:32:43.382587Z","iopub.execute_input":"2024-05-26T08:32:43.382971Z","iopub.status.idle":"2024-05-26T08:32:43.388085Z","shell.execute_reply.started":"2024-05-26T08:32:43.382941Z","shell.execute_reply":"2024-05-26T08:32:43.387203Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"import pickle\nwith open(\"model.pkl\", \"wb\") as file:\n    pickle.dump(m, file)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T08:32:53.301983Z","iopub.execute_input":"2024-05-26T08:32:53.302484Z","iopub.status.idle":"2024-05-26T08:32:53.649014Z","shell.execute_reply.started":"2024-05-26T08:32:53.302443Z","shell.execute_reply":"2024-05-26T08:32:53.648050Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}