{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-05-25T14:52:29.973510Z","iopub.status.busy":"2024-05-25T14:52:29.972670Z","iopub.status.idle":"2024-05-25T15:04:41.445442Z","shell.execute_reply":"2024-05-25T15:04:41.444095Z","shell.execute_reply.started":"2024-05-25T14:52:29.973478Z"},"id":"4cUnD4lfp2hs","trusted":true},"outputs":[{"ename":"NameError","evalue":"name 'torch' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[3], line 75\u001b[0m\n\u001b[1;32m     72\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m GPTTokenizer(\u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m     73\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mtrain(text)\n\u001b[0;32m---> 75\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mtensor(tokenizer\u001b[38;5;241m.\u001b[39mencode(text))\n\u001b[1;32m     76\u001b[0m vocab_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mvocab)\n\u001b[1;32m     78\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m0.8\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"]}],"source":["import regex as re\n","\n","class GPTTokenizer():\n","    def __init__(self, vocab_size):\n","        self.GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n","        self.vocab_size = vocab_size\n","        self.bigram_tree = {}\n","        self.vocab = {idx: bytes([idx]) for idx in range(256)}\n","\n","    def split_text(self, text):\n","        return re.findall(self.GPT4_SPLIT_PATTERN, text)\n","\n","    def freq(self, tokens, stats):\n","        for id1, id2 in zip(tokens, tokens[1:]):\n","            stats[(id1, id2)] = stats.get((id1, id2), 0) + 1\n","\n","    def replace(self, tokens, pair, idx):\n","        newids = []\n","        i = 0\n","        while i < len(tokens):\n","            if tokens[i] == pair[0] and i < len(tokens) - 1 and tokens[i+1] == pair[1]:\n","                newids.append(idx)\n","                i += 2\n","            else:\n","                newids.append(tokens[i])\n","                i += 1\n","        return newids\n","\n","    def train(self, text):\n","        chunks = re.findall(self.GPT4_SPLIT_PATTERN, text)\n","        tokens = [list(chunk.encode('utf-8')) for chunk in chunks]\n","\n","        for i in range(self.vocab_size - 256):\n","            stats = {}\n","            for token in tokens:\n","                self.freq(token, stats)\n","            maxi = max(stats, key=stats.get)\n","            tokens = [self.replace(token, maxi, 256 + i) for token in tokens]\n","            self.bigram_tree[maxi] = 256 + i\n","            self.vocab[256 + i] = self.vocab[maxi[0]] + self.vocab[maxi[1]]\n","\n","    def encode_chunk(self, tokens):\n","        while len(tokens)>=2:\n","            stats = {}\n","            self.freq(tokens, stats)\n","            pair = min(stats, key = lambda p: self.bigram_tree.get(p, float(\"inf\")))\n","            if pair not in self.bigram_tree:\n","                break\n","            tokens = self.replace(tokens, pair, self.bigram_tree[pair])\n","        return tokens\n","\n","    def encode(self, text):\n","        chunks = re.findall(self.GPT4_SPLIT_PATTERN, text)\n","        tokens = []\n","        for chunk in chunks:\n","            tks = list(chunk.encode('utf-8'))\n","            tks = self.encode_chunk(tks)\n","            tokens.extend(tks)\n","\n","        return tokens\n","\n","    def decode(self, tokens):\n","        x = (b\"\".join(self.vocab[idx] for idx in tokens))\n","        text = x.decode('utf-8', errors=\"ignore\")\n","        return text\n","\n","\n","text = \"\"\n","with open(\"/kaggle/input/shakespeare/input.txt\", 'r', encoding = 'utf-8') as f:\n","    text = f.read()\n","\n","tokenizer = GPTTokenizer(1000)\n","tokenizer.train(text)\n","\n","\n"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-05-25T15:06:08.997614Z","iopub.status.busy":"2024-05-25T15:06:08.996666Z","iopub.status.idle":"2024-05-25T15:06:14.558384Z","shell.execute_reply":"2024-05-25T15:06:14.557615Z","shell.execute_reply.started":"2024-05-25T15:06:08.997579Z"},"id":"oPDJUFWg-Rqk","trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import regex as re\n","import torch.optim as optim\n","data = torch.tensor(tokenizer.encode(text))\n","vocab_size = len(tokenizer.vocab)\n","\n","n = int(len(data)*0.8)\n","train = data[:n]\n","valid = data[n:]\n","\n","def get_batch(specify, batch_size, block_size):\n","    data = train if specify == 'train' else valid\n","    idx = torch.randint(len(data)-block_size, (batch_size,))\n","    x = torch.stack([data[i:i+block_size] for i in idx])\n","    y = torch.stack([data[i+1:i+block_size+1] for i in idx])\n","    return x,y\n","# tokenizer.vocab"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-05-25T15:06:26.286235Z","iopub.status.busy":"2024-05-25T15:06:26.285900Z","iopub.status.idle":"2024-05-25T15:06:26.318947Z","shell.execute_reply":"2024-05-25T15:06:26.318007Z","shell.execute_reply.started":"2024-05-25T15:06:26.286209Z"},"id":"isn0SNBmgqoA","trusted":true},"outputs":[],"source":["\n","class EmbeddingLayer(nn.Module):\n","    def __init__(self, d_model, vocab_size, context_length, dropout) -> None:\n","        super().__init__()\n","        self.d_model = d_model\n","        self.vocab_size = vocab_size\n","        self.context_length = context_length\n","        self.inp_embedding = nn.Embedding(self.vocab_size, self.d_model)\n","        self.dropout = nn.Dropout(dropout)\n","        posits = torch.zeros(self.context_length, self.d_model)\n","        position = torch.arange(0, self.context_length, dtype=torch.float).unsqueeze(1)\n","        v_emb = torch.arange(0, self.d_model, 2, dtype=torch.float)\n","\n","        posits[:, 0::2] = torch.sin(position / (10000 ** (v_emb / self.d_model)))\n","        posits[:, 1::2] = torch.cos(position / (10000 ** (v_emb / self.d_model)))\n","\n","        posits = posits.unsqueeze(0)\n","        self.register_buffer('posits', posits)\n","\n","    def forward(self, x):\n","        x = self.inp_embedding(x)\n","        x = x + self.posits[:, :x.size(1), :]\n","        x = self.dropout(x)\n","        return x\n","\n","class ResidualLink(nn.Module):\n","    def __init__(self, sublayer, d_model, dropout):\n","        super().__init__()\n","        self.sublayer = sublayer\n","        self.norm = nn.LayerNorm(d_model)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        return x + self.dropout(self.sublayer(self.norm(x)))\n","\n","class CausalSelfAttention(nn.Module):\n","    def __init__(self, d_model, head_size, dropout) -> None:\n","        super().__init__()\n","        self.d_model = d_model\n","        self.head_size = head_size\n","        self.dropout = nn.Dropout(dropout)\n","        self.query = nn.Linear(self.d_model, self.head_size)\n","        self.key = nn.Linear(self.d_model, self.head_size)\n","        self.value = nn.Linear(self.d_model, self.head_size)\n","\n","    def forward(self, x):\n","        B, T, C = x.shape\n","        q = self.query(x)\n","        k = self.key(x)\n","        v = self.value(x)\n","        out1 = q @ k.transpose(-2, -1) / (self.head_size ** 0.5)\n","        mask = torch.tril(torch.ones(T, T)).to(x.device)\n","        out1 = out1.masked_fill(mask == 0, float('-inf'))\n","        out1 = F.softmax(out1, dim=-1)\n","        out1 = self.dropout(out1)\n","        out2 = out1 @ v\n","        return out2\n","\n","class MultiHeadAttention(nn.Module):\n","    def __init__(self, d_model, num_heads, dropout) -> None:\n","        super().__init__()\n","        self.d_model = d_model\n","        self.num_heads = num_heads\n","        self.dropout = nn.Dropout(dropout)\n","        self.head_size = self.d_model // self.num_heads\n","        self.attention_heads = nn.ModuleList([CausalSelfAttention(self.d_model, self.head_size, dropout) for _ in range(self.num_heads)])\n","        self.w_out = nn.Linear(self.d_model, self.d_model)\n","\n","    def forward(self, x):\n","        self.out = torch.cat([head(x) for head in self.attention_heads], dim=-1)\n","        self.out = self.w_out(self.out)\n","        self.out = self.dropout(self.out)\n","        return self.out\n","\n","class FeedForwardNetwork(nn.Module):\n","    def __init__(self, d_model, dropout) -> None:\n","        super().__init__()\n","        self.d_model = d_model\n","        self.d_ff = d_model * 4\n","        self.dropout = nn.Dropout(dropout)\n","        self.linear1 = nn.Linear(self.d_model, self.d_ff)\n","        self.linear2 = nn.Linear(self.d_ff, self.d_model)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","        out = self.relu(self.linear1(x))\n","        out = self.linear2(out)\n","        out = self.dropout(out)\n","        return out\n","\n","class DecoderBlock(nn.Module):\n","    def __init__(self, d_model, num_heads, dropout) -> None:\n","        super().__init__()\n","        self.d_model = d_model\n","        self.num_heads = num_heads\n","        self.dropout = nn.Dropout(dropout)\n","        self.attention = ResidualLink(MultiHeadAttention(self.d_model, self.num_heads, dropout), self.d_model, dropout)\n","        self.feedforward = ResidualLink(FeedForwardNetwork(self.d_model, dropout), self.d_model, dropout)\n","\n","    def forward(self, x):\n","        x = self.attention(x)\n","        x = self.feedforward(x)\n","        return x\n","\n","class GPTBlock(nn.Module):\n","    def __init__(self, vocab_size, d_model, context_length, n_decoder_blocks, dropout) -> None:\n","        super().__init__()\n","        self.embeds = EmbeddingLayer(d_model, vocab_size, context_length, dropout)\n","        self.decoder_blocks = nn.ModuleList([DecoderBlock(d_model, 8, dropout) for _ in range(n_decoder_blocks)])\n","        self.norm = nn.LayerNorm(d_model)\n","        self.linear = nn.Linear(d_model, vocab_size)\n","\n","    def forward(self, x):\n","        x = self.embeds(x)\n","        for block in self.decoder_blocks:\n","            x = block(x)\n","        x = self.norm(x)\n","        x = self.linear(x)\n","        return x\n","\n","class BigramLM(nn.Module):\n","    def __init__(self, context_length, n_decoder_blocks, dropout, vocab_size=1000, d_model=512):\n","        super().__init__()\n","        self.vocab_size = vocab_size\n","        self.d_model = d_model\n","        self.model = GPTBlock(vocab_size, d_model, context_length, n_decoder_blocks, dropout)\n","        self.context_length = context_length\n","\n","    def forward(self, x, targets=None):\n","        x = x.cuda()\n","        logits = self.model(x)\n","        loss = None\n","        if targets is not None:\n","            targets = targets.cuda()\n","            logits = logits.view(-1, self.vocab_size)\n","            targets = targets.view(-1)\n","            loss = F.cross_entropy(logits, targets)\n","        return logits, loss\n","\n","    @torch.no_grad()\n","    def generate(self, x, num_char=1000):\n","        for i in range(num_char):\n","            x_cond = x[:, -self.context_length:]\n","            pred, _ = self(x_cond)\n","            preds = pred[:, -1, :]\n","            probs = F.softmax(preds, dim=-1)\n","            idx = torch.multinomial(probs, num_samples=1)\n","            x = torch.cat((x, idx), dim=1)\n","        return x"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-05-25T15:06:28.816864Z","iopub.status.busy":"2024-05-25T15:06:28.816490Z","iopub.status.idle":"2024-05-25T15:06:28.821567Z","shell.execute_reply":"2024-05-25T15:06:28.820568Z","shell.execute_reply.started":"2024-05-25T15:06:28.816832Z"},"id":"8oxcVcasxyYU","trusted":true},"outputs":[],"source":["context_length = 32\n","n_decoder_blocks = 8\n","batch_size = 64\n","eval_iters = 20\n","eval_interval = 2000\n","dropout = 0.3"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-05-25T15:24:27.303505Z","iopub.status.busy":"2024-05-25T15:24:27.302591Z","iopub.status.idle":"2024-05-25T15:24:27.632577Z","shell.execute_reply":"2024-05-25T15:24:27.631583Z","shell.execute_reply.started":"2024-05-25T15:24:27.303469Z"},"id":"NCOxU6-bxJLE","trusted":true},"outputs":[],"source":["m = BigramLM(context_length, n_decoder_blocks, dropout)\n","m = m.cuda()"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-05-25T15:24:27.752492Z","iopub.status.busy":"2024-05-25T15:24:27.751768Z","iopub.status.idle":"2024-05-25T15:24:27.758162Z","shell.execute_reply":"2024-05-25T15:24:27.757246Z","shell.execute_reply.started":"2024-05-25T15:24:27.752463Z"},"id":"_bEaOB3l0t_X","trusted":true},"outputs":[],"source":["@torch.no_grad()\n","def estimate_loss():\n","    out = {}\n","    m.eval()\n","    for split in ['train', 'val']:\n","        losses = torch.zeros(eval_iters)\n","        for k in range(eval_iters):\n","            X, Y = get_batch(split,batch_size,context_length)\n","            logits, loss = m(X, Y)\n","            losses[k] = loss.item()\n","        out[split] = losses.mean()\n","    m.train()\n","    return out"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-05-25T15:24:30.487685Z","iopub.status.busy":"2024-05-25T15:24:30.486970Z","iopub.status.idle":"2024-05-25T15:24:30.494870Z","shell.execute_reply":"2024-05-25T15:24:30.493843Z","shell.execute_reply.started":"2024-05-25T15:24:30.487654Z"},"id":"FiglBzQ0xLdu","outputId":"d2bb5c83-0a26-473e-f164-8d08fb6e3fb7","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["26.245096 M parameters\n"]}],"source":["print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":435},"execution":{"iopub.execute_input":"2024-05-25T15:24:32.328302Z","iopub.status.busy":"2024-05-25T15:24:32.327383Z","iopub.status.idle":"2024-05-25T15:46:15.639384Z","shell.execute_reply":"2024-05-25T15:46:15.638126Z","shell.execute_reply.started":"2024-05-25T15:24:32.328242Z"},"id":"W0F-Y-VpxSv-","outputId":"48415014-0788-432e-b97f-fdf0edbb99b1","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["step 0: train loss 7.0774, val loss 7.0801\n","step 2000: train loss 3.0474, val loss 4.1226\n","step 4000: train loss 2.6172, val loss 4.1959\n","step 6000: train loss 2.1963, val loss 4.3641\n","step 8000: train loss 1.8655, val loss 4.5564\n","step 10000: train loss 1.5771, val loss 4.7767\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[18], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: train loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, val loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m opt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 10\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m opt\u001b[38;5;241m.\u001b[39mstep()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["opt = optim.Adam(m.parameters(), lr = 6e-4)\n","\n","for i in range(50000):\n","    x,y = get_batch('train',batch_size, context_length)\n","    preds,loss = m(x,y)\n","    if i % eval_interval == 0:\n","        losses = estimate_loss()\n","        print(f\"step {i}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n","    opt.zero_grad()\n","    loss.backward()\n","    opt.step()\n"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":282},"execution":{"iopub.execute_input":"2024-05-25T15:46:19.422255Z","iopub.status.busy":"2024-05-25T15:46:19.421509Z","iopub.status.idle":"2024-05-25T15:47:17.450244Z","shell.execute_reply":"2024-05-25T15:47:17.449197Z","shell.execute_reply.started":"2024-05-25T15:46:19.422224Z"},"id":"Cu2nerWrsoJP","outputId":"4a75e1c9-9613-4fd2-dc70-8b1f8f494311","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u0000 that most glad. I'll not have done't again.\n","\n","ANTIGONUS:\n","What you have beower to be of than you and told on't,\n","Like a surcept after horse that swe thee?\n","\n","JOHN OF GAUNT:\n","Give me my lie.\n","\n","MERCUTIO:\n","My prayers, go with me, where I should not trouble you. I\n","Appear forth all my heart.\n","Come you, coward my heart disdent and griedyvery.\n","\n","JULIET:\n","You have some said to kiss her life away?\n","I'll make you glp her bosom in't\n","More bastards play another, for thou know these way:\n","I have all mock'd for ever.'\n","\n","AUTOLYCUS:\n","Give me my leave, I beseech you!\n","I'll to you, sir, temper thee, for you those that knows\n","justified.\n","\n","Father:\n","And would it were a Richard's best friend, which the\n","sues\n","That thou hast made'st a traitor's head by\n","Diss and by joces thereof;\n","For God will I'll do my grief, my vow unto mine honour,\n","Be to thee reverend me stop wars,\n","But what rests changed me with living nimb;\n","Feather weeel wink first to see you, brother Montague,\n","And stops ' feel water for thee: if pay him,\n","Alack houses, with aunt age hell of breath\n","As ever first sined\n","To witness about her hour. The Capule's tears\n","Do homely bones, that they killed's to have\n","By one sole on the other: hot's we changed\n","He has not named for that terror, comfort,\n","And tells the king, who devise him.\n","\n","CLARENCE:\n","It shall be so, I'll give this kindingly may case.\n","Unless death shall call thee marry woe for Iris,\n","Tell me in reversign of commons' kings,\n","And not live we free him, but notself,\n","Before the jintius to me, and the senate poll,\n","Whose deceiving horse that I pay my stretch\n","As shall my general terms to the light-fold o'\n","More fearing on the match, to give me thiece:\n","Som on a soldier's sun, king, and his bed,\n","Have took a petition of joys to want from him.\n","What, thinking how he nothing?\n","\n","CATESBY:\n","As any colour to the trial guests!\n","Give me my friend, and cause: and my methinks;\n","Name I to every tongue give itself and victory.\n","Upon the wondrous massion of a villanous o'\n","As if it may be married to be; for not it is but a small--henust\n","for the next night betwixt them rushes, even ride!\n","The kindred of his blood rememane.\n","\n","KING RICHARD II:\n","O my gages, boots? strifs, sy nose thousand hearts must errong?\n","eath, at thy advice, and these fThird Mabercy\n","Was your ignoffer wrong, or else\n","For me brave councils, Livalent fond.\n","Now, own you talken days to hate me of life.\n","If you do, beseech you, take it, looks offence,\n","Unfor a like measure for lesser than age;\n","I think it is: and Romeo is my name\n","I'll not remember that name:\n","Henry forfeits, and bats both of runs;\n","The raven doth appear to their formsocrifice.\n","An earth, poor muckeal to statelys\n","I' the chiefest m seeing-ford feed upon\n","This last to her chol on her. But make her,\n","Would you must not keep betake a body, or up,\n","To whom deny his imperoral banishment.\n","\n","SOMERSET:\n","O Hark ye he daughter! when I kiss my niece,.\n","God save us to Haryries!\n","\n","QUEEN MARGARET:\n","PHavity! let me yet to make thee live.\n","\n","GLOUCESTER:\n","Tell not, try one that speed time to want,\n","He issued true and Hermione hath supposed;\n","And Isabel a happy into a silverge,\n","And next speak more than my soul I do protest,\n","Ere he can make thee safe, if I please to go to go with fire.\n","\n","VIRGILIA:\n","To sleight of sympent King Edward's death,\n","To two ill unto my hide; horse is no slave,\n","And let him die, even in your own joys as\n","You kill the blood of heaven: were it lived it light\n","To bear his light again upon you, and so bull of stune,\n","As little offence, set up Lontes shall not be acreant;\n","And, now she is that she was coming to this title's grace.\n","\n","GLOUCESTER:\n","Faith, most dungeishonour lieys it: it may it:\n","Methinks I will see the courtesy field.\n","\n","GREEN:\n","Come between us we meet again; at every one part,\n","Who slew o'er the crow to beggars to see him;\n","And he then was stabbuten'd to one--I pity--,\n","O, no, I know, I have with Membray'd disploys.\n","Ten days, the good old fem Capulets levied G;\n","And take me thrive I seek that his former worth:\n","How should you so, my my lord?\n","\n","KING RICHARD III:\n","That unnatural Nay, if you be a common son.\n","\n","BUCKINGHAM:\n","I'll to the king shall espy.\n","\n","KING RICHARD III:\n","And besides, he seems you to use this deed?\n","Fromly to an you, and my poor household choice?\n","\n","CLARENCE:\n","Why, then, you kiss your sword, I'll stood\n","With male tigony on my shoulders: but we mean,\n","May lead upon thine own deep, and my griefs,\n","It doth not reward my grievous shiegeful we's banishment!\n","And if 'twerest be seen to make no wars.\n","\n","CLAUDIO:\n","Who's thy garments between us: remember our good cer\n","Hves as savour of royal house. Put him both him,\n","Without his friend-prins more grave than\n","Than needful this mine. For end your largeity?\n","Force me, sound more than a husband's charters from thy\n","malice. There's not in these confidence I,\n","Or bids us blow my tread, our spring in rest!\n","Thoughts old burilding high postserterse and fearns all;\n","That look upon themselves, many make no privy miseries\n","What custom of it is, and being but a gentlewry's\n","And stir the navish captain would resemble.\n","\n","CLIFFORD:\n","He that dares not for Edward hast thou!\n","\n","CLARENCE:\n","Burther than mercy, when he commends untend their souls,\n","The crown'd your brothers, and Talker:\n","I hold you by the hands of this banish'd man,\n","Clares fa\n"]}],"source":["print(tokenizer.decode(list(map(int, m.generate(torch.zeros((1,1), dtype = torch.long).cuda(), 2000)[0]))))"]},{"cell_type":"code","execution_count":68,"metadata":{"id":"dN3TIeE0wF3C"},"outputs":[],"source":["x = torch.zeros((1,1))"]},{"cell_type":"code","execution_count":74,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"On4AUF5qGWZj","outputId":"d47be6cd-fbbd-4033-bc8c-4334a60e4a10"},"outputs":[{"data":{"text/plain":["torch.Size([30, 512])"]},"execution_count":74,"metadata":{},"output_type":"execute_result"}],"source":["(x + torch.zeros(30, 512)).shape"]},{"cell_type":"code","execution_count":87,"metadata":{"id":"_dfLZCEKMxDb"},"outputs":[],"source":["preds,_ = m(x)"]},{"cell_type":"code","execution_count":89,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RdCiIN2pM7Cl","outputId":"7f43219c-d058-4b93-a62c-0d97ebf4c249"},"outputs":[{"data":{"text/plain":["torch.Size([64, 32, 1000])"]},"execution_count":89,"metadata":{},"output_type":"execute_result"}],"source":["preds.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8mAutb3YM7uM"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5083566,"sourceId":8514979,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
