{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8514979,"sourceType":"datasetVersion","datasetId":5083566}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import regex as re\n\nclass GPTTokenizer():\n    def __init__(self, vocab_size):\n        self.GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n        self.vocab_size = vocab_size\n        self.bigram_tree = {}\n        self.vocab = {idx: bytes([idx]) for idx in range(256)}\n\n    def split_text(self, text):\n        return re.findall(self.GPT4_SPLIT_PATTERN, text)\n\n    def freq(self, tokens, stats):\n        for id1, id2 in zip(tokens, tokens[1:]):\n            stats[(id1, id2)] = stats.get((id1, id2), 0) + 1\n\n    def replace(self, tokens, pair, idx):\n        newids = []\n        i = 0\n        while i < len(tokens):\n            if tokens[i] == pair[0] and i < len(tokens) - 1 and tokens[i+1] == pair[1]:\n                newids.append(idx)\n                i += 2\n            else:\n                newids.append(tokens[i])\n                i += 1\n        return newids\n\n    def train(self, text):\n        chunks = re.findall(self.GPT4_SPLIT_PATTERN, text)\n        tokens = [list(chunk.encode('utf-8')) for chunk in chunks]\n\n        for i in range(self.vocab_size - 256):\n            stats = {}\n            for token in tokens:\n                self.freq(token, stats)\n            maxi = max(stats, key=stats.get)\n            tokens = [self.replace(token, maxi, 256 + i) for token in tokens]\n            self.bigram_tree[maxi] = 256 + i\n            self.vocab[256 + i] = self.vocab[maxi[0]] + self.vocab[maxi[1]]\n\n    def encode_chunk(self, tokens):\n        while len(tokens)>=2:\n            stats = {}\n            self.freq(tokens, stats)\n            pair = min(stats, key = lambda p: self.bigram_tree.get(p, float(\"inf\")))\n            if pair not in self.bigram_tree:\n                break\n            tokens = self.replace(tokens, pair, self.bigram_tree[pair])\n        return tokens\n\n    def encode(self, text):\n        chunks = re.findall(self.GPT4_SPLIT_PATTERN, text)\n        tokens = []\n        for chunk in chunks:\n            tks = list(chunk.encode('utf-8'))\n            tks = self.encode_chunk(tks)\n            tokens.extend(tks)\n\n        return tokens\n\n    def decode(self, tokens):\n        x = (b\"\".join(self.vocab[idx] for idx in tokens))\n        text = x.decode('utf-8', errors=\"ignore\")\n        return text\n\n\ntext = \"\"\nwith open(\"/kaggle/input/shakespeare/input.txt\", 'r', encoding = 'utf-8') as f:\n    text = f.read()\n\ntokenizer = GPTTokenizer(1000)\ntokenizer.train(text)\n\n\n","metadata":{"id":"4cUnD4lfp2hs","execution":{"iopub.status.busy":"2024-05-25T14:52:29.972670Z","iopub.execute_input":"2024-05-25T14:52:29.973510Z","iopub.status.idle":"2024-05-25T15:04:41.445442Z","shell.execute_reply.started":"2024-05-25T14:52:29.973478Z","shell.execute_reply":"2024-05-25T15:04:41.444095Z"},"trusted":true},"execution_count":3,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[3], line 75\u001b[0m\n\u001b[1;32m     72\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m GPTTokenizer(\u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m     73\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mtrain(text)\n\u001b[0;32m---> 75\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mtensor(tokenizer\u001b[38;5;241m.\u001b[39mencode(text))\n\u001b[1;32m     76\u001b[0m vocab_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mvocab)\n\u001b[1;32m     78\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m0.8\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"],"ename":"NameError","evalue":"name 'torch' is not defined","output_type":"error"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport regex as re\nimport torch.optim as optim\ndata = torch.tensor(tokenizer.encode(text))\nvocab_size = len(tokenizer.vocab)\n\nn = int(len(data)*0.8)\ntrain = data[:n]\nvalid = data[n:]\n\ndef get_batch(specify, batch_size, block_size):\n    data = train if specify == 'train' else valid\n    idx = torch.randint(len(data)-block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in idx])\n    y = torch.stack([data[i+1:i+block_size+1] for i in idx])\n    return x,y\n# tokenizer.vocab","metadata":{"id":"oPDJUFWg-Rqk","execution":{"iopub.status.busy":"2024-05-25T15:06:08.996666Z","iopub.execute_input":"2024-05-25T15:06:08.997614Z","iopub.status.idle":"2024-05-25T15:06:14.558384Z","shell.execute_reply.started":"2024-05-25T15:06:08.997579Z","shell.execute_reply":"2024-05-25T15:06:14.557615Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"\nclass EmbeddingLayer(nn.Module):\n    def __init__(self, d_model, vocab_size, context_length, dropout) -> None:\n        super().__init__()\n        self.d_model = d_model\n        self.vocab_size = vocab_size\n        self.context_length = context_length\n        self.inp_embedding = nn.Embedding(self.vocab_size, self.d_model)\n        self.dropout = nn.Dropout(dropout)\n        posits = torch.zeros(self.context_length, self.d_model)\n        position = torch.arange(0, self.context_length, dtype=torch.float).unsqueeze(1)\n        v_emb = torch.arange(0, self.d_model, 2, dtype=torch.float)\n\n        posits[:, 0::2] = torch.sin(position / (10000 ** (v_emb / self.d_model)))\n        posits[:, 1::2] = torch.cos(position / (10000 ** (v_emb / self.d_model)))\n\n        posits = posits.unsqueeze(0)\n        self.register_buffer('posits', posits)\n\n    def forward(self, x):\n        x = self.inp_embedding(x)\n        x = x + self.posits[:, :x.size(1), :]\n        x = self.dropout(x)\n        return x\n\nclass ResidualLink(nn.Module):\n    def __init__(self, sublayer, d_model, dropout):\n        super().__init__()\n        self.sublayer = sublayer\n        self.norm = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        return x + self.dropout(self.sublayer(self.norm(x)))\n\nclass CausalSelfAttention(nn.Module):\n    def __init__(self, d_model, head_size, dropout) -> None:\n        super().__init__()\n        self.d_model = d_model\n        self.head_size = head_size\n        self.dropout = nn.Dropout(dropout)\n        self.query = nn.Linear(self.d_model, self.head_size)\n        self.key = nn.Linear(self.d_model, self.head_size)\n        self.value = nn.Linear(self.d_model, self.head_size)\n\n    def forward(self, x):\n        B, T, C = x.shape\n        q = self.query(x)\n        k = self.key(x)\n        v = self.value(x)\n        out1 = q @ k.transpose(-2, -1) / (self.head_size ** 0.5)\n        mask = torch.tril(torch.ones(T, T)).to(x.device)\n        out1 = out1.masked_fill(mask == 0, float('-inf'))\n        out1 = F.softmax(out1, dim=-1)\n        out1 = self.dropout(out1)\n        out2 = out1 @ v\n        return out2\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, num_heads, dropout) -> None:\n        super().__init__()\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.dropout = nn.Dropout(dropout)\n        self.head_size = self.d_model // self.num_heads\n        self.attention_heads = nn.ModuleList([CausalSelfAttention(self.d_model, self.head_size, dropout) for _ in range(self.num_heads)])\n        self.w_out = nn.Linear(self.d_model, self.d_model)\n\n    def forward(self, x):\n        self.out = torch.cat([head(x) for head in self.attention_heads], dim=-1)\n        self.out = self.w_out(self.out)\n        self.out = self.dropout(self.out)\n        return self.out\n\nclass FeedForwardNetwork(nn.Module):\n    def __init__(self, d_model, dropout) -> None:\n        super().__init__()\n        self.d_model = d_model\n        self.d_ff = d_model * 4\n        self.dropout = nn.Dropout(dropout)\n        self.linear1 = nn.Linear(self.d_model, self.d_ff)\n        self.linear2 = nn.Linear(self.d_ff, self.d_model)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        out = self.relu(self.linear1(x))\n        out = self.linear2(out)\n        out = self.dropout(out)\n        return out\n\nclass DecoderBlock(nn.Module):\n    def __init__(self, d_model, num_heads, dropout) -> None:\n        super().__init__()\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.dropout = nn.Dropout(dropout)\n        self.attention = ResidualLink(MultiHeadAttention(self.d_model, self.num_heads, dropout), self.d_model, dropout)\n        self.feedforward = ResidualLink(FeedForwardNetwork(self.d_model, dropout), self.d_model, dropout)\n\n    def forward(self, x):\n        x = self.attention(x)\n        x = self.feedforward(x)\n        return x\n\nclass GPTBlock(nn.Module):\n    def __init__(self, vocab_size, d_model, context_length, n_decoder_blocks, dropout) -> None:\n        super().__init__()\n        self.embeds = EmbeddingLayer(d_model, vocab_size, context_length, dropout)\n        self.decoder_blocks = nn.ModuleList([DecoderBlock(d_model, 8, dropout) for _ in range(n_decoder_blocks)])\n        self.norm = nn.LayerNorm(d_model)\n        self.linear = nn.Linear(d_model, vocab_size)\n\n    def forward(self, x):\n        x = self.embeds(x)\n        for block in self.decoder_blocks:\n            x = block(x)\n        x = self.norm(x)\n        x = self.linear(x)\n        return x\n\nclass BigramLM(nn.Module):\n    def __init__(self, context_length, n_decoder_blocks, dropout, vocab_size=1000, d_model=512):\n        super().__init__()\n        self.vocab_size = vocab_size\n        self.d_model = d_model\n        self.model = GPTBlock(vocab_size, d_model, context_length, n_decoder_blocks, dropout)\n        self.context_length = context_length\n\n    def forward(self, x, targets=None):\n        x = x.cuda()\n        logits = self.model(x)\n        loss = None\n        if targets is not None:\n            targets = targets.cuda()\n            logits = logits.view(-1, self.vocab_size)\n            targets = targets.view(-1)\n            loss = F.cross_entropy(logits, targets)\n        return logits, loss\n\n    @torch.no_grad()\n    def generate(self, x, num_char=1000):\n        for i in range(num_char):\n            x_cond = x[:, -self.context_length:]\n            pred, _ = self(x_cond)\n            preds = pred[:, -1, :]\n            probs = F.softmax(preds, dim=-1)\n            idx = torch.multinomial(probs, num_samples=1)\n            x = torch.cat((x, idx), dim=1)\n        return x","metadata":{"id":"isn0SNBmgqoA","execution":{"iopub.status.busy":"2024-05-25T15:06:26.285900Z","iopub.execute_input":"2024-05-25T15:06:26.286235Z","iopub.status.idle":"2024-05-25T15:06:26.318947Z","shell.execute_reply.started":"2024-05-25T15:06:26.286209Z","shell.execute_reply":"2024-05-25T15:06:26.318007Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"context_length = 32\nn_decoder_blocks = 8\nbatch_size = 64\neval_iters = 20\neval_interval = 2000\ndropout = 0.3","metadata":{"id":"8oxcVcasxyYU","execution":{"iopub.status.busy":"2024-05-25T15:06:28.816490Z","iopub.execute_input":"2024-05-25T15:06:28.816864Z","iopub.status.idle":"2024-05-25T15:06:28.821567Z","shell.execute_reply.started":"2024-05-25T15:06:28.816832Z","shell.execute_reply":"2024-05-25T15:06:28.820568Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"m = BigramLM(context_length, n_decoder_blocks, dropout)\nm = m.cuda()","metadata":{"id":"NCOxU6-bxJLE","execution":{"iopub.status.busy":"2024-05-25T15:24:27.302591Z","iopub.execute_input":"2024-05-25T15:24:27.303505Z","iopub.status.idle":"2024-05-25T15:24:27.632577Z","shell.execute_reply.started":"2024-05-25T15:24:27.303469Z","shell.execute_reply":"2024-05-25T15:24:27.631583Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    m.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split,batch_size,context_length)\n            logits, loss = m(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    m.train()\n    return out","metadata":{"id":"_bEaOB3l0t_X","execution":{"iopub.status.busy":"2024-05-25T15:24:27.751768Z","iopub.execute_input":"2024-05-25T15:24:27.752492Z","iopub.status.idle":"2024-05-25T15:24:27.758162Z","shell.execute_reply.started":"2024-05-25T15:24:27.752463Z","shell.execute_reply":"2024-05-25T15:24:27.757246Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FiglBzQ0xLdu","outputId":"d2bb5c83-0a26-473e-f164-8d08fb6e3fb7","execution":{"iopub.status.busy":"2024-05-25T15:24:30.486970Z","iopub.execute_input":"2024-05-25T15:24:30.487685Z","iopub.status.idle":"2024-05-25T15:24:30.494870Z","shell.execute_reply.started":"2024-05-25T15:24:30.487654Z","shell.execute_reply":"2024-05-25T15:24:30.493843Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"26.245096 M parameters\n","output_type":"stream"}]},{"cell_type":"code","source":"opt = optim.Adam(m.parameters(), lr = 6e-4)\n\nfor i in range(50000):\n    x,y = get_batch('train',batch_size, context_length)\n    preds,loss = m(x,y)\n    if i % eval_interval == 0:\n        losses = estimate_loss()\n        print(f\"step {i}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n    opt.zero_grad()\n    loss.backward()\n    opt.step()\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":435},"id":"W0F-Y-VpxSv-","outputId":"48415014-0788-432e-b97f-fdf0edbb99b1","execution":{"iopub.status.busy":"2024-05-25T15:24:32.327383Z","iopub.execute_input":"2024-05-25T15:24:32.328302Z","iopub.status.idle":"2024-05-25T15:46:15.639384Z","shell.execute_reply.started":"2024-05-25T15:24:32.328242Z","shell.execute_reply":"2024-05-25T15:46:15.638126Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"step 0: train loss 7.0774, val loss 7.0801\nstep 2000: train loss 3.0474, val loss 4.1226\nstep 4000: train loss 2.6172, val loss 4.1959\nstep 6000: train loss 2.1963, val loss 4.3641\nstep 8000: train loss 1.8655, val loss 4.5564\nstep 10000: train loss 1.5771, val loss 4.7767\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[18], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: train loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, val loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m opt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 10\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m opt\u001b[38;5;241m.\u001b[39mstep()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"print(tokenizer.decode(list(map(int, m.generate(torch.zeros((1,1), dtype = torch.long).cuda(), 2000)[0]))))","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":282},"id":"Cu2nerWrsoJP","outputId":"4a75e1c9-9613-4fd2-dc70-8b1f8f494311","execution":{"iopub.status.busy":"2024-05-25T15:46:19.421509Z","iopub.execute_input":"2024-05-25T15:46:19.422255Z","iopub.status.idle":"2024-05-25T15:47:17.450244Z","shell.execute_reply.started":"2024-05-25T15:46:19.422224Z","shell.execute_reply":"2024-05-25T15:47:17.449197Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"\u0000 that most glad. I'll not have done't again.\n\nANTIGONUS:\nWhat you have beower to be of than you and told on't,\nLike a surcept after horse that swe thee?\n\nJOHN OF GAUNT:\nGive me my lie.\n\nMERCUTIO:\nMy prayers, go with me, where I should not trouble you. I\nAppear forth all my heart.\nCome you, coward my heart disdent and griedyvery.\n\nJULIET:\nYou have some said to kiss her life away?\nI'll make you glp her bosom in't\nMore bastards play another, for thou know these way:\nI have all mock'd for ever.'\n\nAUTOLYCUS:\nGive me my leave, I beseech you!\nI'll to you, sir, temper thee, for you those that knows\njustified.\n\nFather:\nAnd would it were a Richard's best friend, which the\nsues\nThat thou hast made'st a traitor's head by\nDiss and by joces thereof;\nFor God will I'll do my grief, my vow unto mine honour,\nBe to thee reverend me stop wars,\nBut what rests changed me with living nimb;\nFeather weeel wink first to see you, brother Montague,\nAnd stops ' feel water for thee: if pay him,\nAlack houses, with aunt age hell of breath\nAs ever first sined\nTo witness about her hour. The Capule's tears\nDo homely bones, that they killed's to have\nBy one sole on the other: hot's we changed\nHe has not named for that terror, comfort,\nAnd tells the king, who devise him.\n\nCLARENCE:\nIt shall be so, I'll give this kindingly may case.\nUnless death shall call thee marry woe for Iris,\nTell me in reversign of commons' kings,\nAnd not live we free him, but notself,\nBefore the jintius to me, and the senate poll,\nWhose deceiving horse that I pay my stretch\nAs shall my general terms to the light-fold o'\nMore fearing on the match, to give me thiece:\nSom on a soldier's sun, king, and his bed,\nHave took a petition of joys to want from him.\nWhat, thinking how he nothing?\n\nCATESBY:\nAs any colour to the trial guests!\nGive me my friend, and cause: and my methinks;\nName I to every tongue give itself and victory.\nUpon the wondrous massion of a villanous o'\nAs if it may be married to be; for not it is but a small--henust\nfor the next night betwixt them rushes, even ride!\nThe kindred of his blood rememane.\n\nKING RICHARD II:\nO my gages, boots? strifs, sy nose thousand hearts must errong?\neath, at thy advice, and these fThird Mabercy\nWas your ignoffer wrong, or else\nFor me brave councils, Livalent fond.\nNow, own you talken days to hate me of life.\nIf you do, beseech you, take it, looks offence,\nUnfor a like measure for lesser than age;\nI think it is: and Romeo is my name\nI'll not remember that name:\nHenry forfeits, and bats both of runs;\nThe raven doth appear to their formsocrifice.\nAn earth, poor muckeal to statelys\nI' the chiefest m seeing-ford feed upon\nThis last to her chol on her. But make her,\nWould you must not keep betake a body, or up,\nTo whom deny his imperoral banishment.\n\nSOMERSET:\nO Hark ye he daughter! when I kiss my niece,.\nGod save us to Haryries!\n\nQUEEN MARGARET:\nPHavity! let me yet to make thee live.\n\nGLOUCESTER:\nTell not, try one that speed time to want,\nHe issued true and Hermione hath supposed;\nAnd Isabel a happy into a silverge,\nAnd next speak more than my soul I do protest,\nEre he can make thee safe, if I please to go to go with fire.\n\nVIRGILIA:\nTo sleight of sympent King Edward's death,\nTo two ill unto my hide; horse is no slave,\nAnd let him die, even in your own joys as\nYou kill the blood of heaven: were it lived it light\nTo bear his light again upon you, and so bull of stune,\nAs little offence, set up Lontes shall not be acreant;\nAnd, now she is that she was coming to this title's grace.\n\nGLOUCESTER:\nFaith, most dungeishonour lieys it: it may it:\nMethinks I will see the courtesy field.\n\nGREEN:\nCome between us we meet again; at every one part,\nWho slew o'er the crow to beggars to see him;\nAnd he then was stabbuten'd to one--I pity--,\nO, no, I know, I have with Membray'd disploys.\nTen days, the good old fem Capulets levied G;\nAnd take me thrive I seek that his former worth:\nHow should you so, my my lord?\n\nKING RICHARD III:\nThat unnatural Nay, if you be a common son.\n\nBUCKINGHAM:\nI'll to the king shall espy.\n\nKING RICHARD III:\nAnd besides, he seems you to use this deed?\nFromly to an you, and my poor household choice?\n\nCLARENCE:\nWhy, then, you kiss your sword, I'll stood\nWith male tigony on my shoulders: but we mean,\nMay lead upon thine own deep, and my griefs,\nIt doth not reward my grievous shiegeful we's banishment!\nAnd if 'twerest be seen to make no wars.\n\nCLAUDIO:\nWho's thy garments between us: remember our good cer\nHves as savour of royal house. Put him both him,\nWithout his friend-prins more grave than\nThan needful this mine. For end your largeity?\nForce me, sound more than a husband's charters from thy\nmalice. There's not in these confidence I,\nOr bids us blow my tread, our spring in rest!\nThoughts old burilding high postserterse and fearns all;\nThat look upon themselves, many make no privy miseries\nWhat custom of it is, and being but a gentlewry's\nAnd stir the navish captain would resemble.\n\nCLIFFORD:\nHe that dares not for Edward hast thou!\n\nCLARENCE:\nBurther than mercy, when he commends untend their souls,\nThe crown'd your brothers, and Talker:\nI hold you by the hands of this banish'd man,\nClares fa\n","output_type":"stream"}]},{"cell_type":"code","source":"x = torch.zeros((1,1))","metadata":{"id":"dN3TIeE0wF3C"},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"(x + torch.zeros(30, 512)).shape","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"On4AUF5qGWZj","outputId":"d47be6cd-fbbd-4033-bc8c-4334a60e4a10"},"execution_count":74,"outputs":[{"output_type":"execute_result","execution_count":74,"data":{"text/plain":["torch.Size([30, 512])"]},"metadata":{}}]},{"cell_type":"code","source":"preds,_ = m(x)","metadata":{"id":"_dfLZCEKMxDb"},"execution_count":87,"outputs":[]},{"cell_type":"code","source":"preds.shape","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RdCiIN2pM7Cl","outputId":"7f43219c-d058-4b93-a62c-0d97ebf4c249"},"execution_count":89,"outputs":[{"output_type":"execute_result","execution_count":89,"data":{"text/plain":["torch.Size([64, 32, 1000])"]},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"id":"8mAutb3YM7uM"},"execution_count":null,"outputs":[]}]}
